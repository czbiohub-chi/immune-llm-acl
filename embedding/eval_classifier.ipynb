{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../benchmarks\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from benchmark import load_benchmark\n",
    "from src.mlp import MLP\n",
    "\n",
    "api_url = \"https://api.openai.com/v1\"\n",
    "api_key = input(\"OPENAI_API_KEY\")\n",
    "summary_model = \"gpt-4o-2024-11-20\"\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "benchmark_path = Path(\"../benchmarks/benchmark-difficult\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = load_benchmark(benchmark_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings (of Summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_emb_map = {\n",
    "    \"mouse\": np.load(\"data/embeddings/genes_mouse.npy\", allow_pickle=True).item(),\n",
    "    \"human\": np.load(\"data/embeddings/genes_human.npy\", allow_pickle=True).item(),\n",
    "}\n",
    "summ_gene_emb_map = {\n",
    "    \"mouse\": np.load(\"data/embeddings/summarized_genes_mouse.npy\", allow_pickle=True).item(),\n",
    "    \"human\": np.load(\"data/embeddings/summarized_genes_human.npy\", allow_pickle=True).item(),\n",
    "}\n",
    "method_emb = np.load(\"data/embeddings/methods.npy\", allow_pickle=True).item()\n",
    "summ_method_emb = np.load(\"data/embeddings/summarized_methods.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = benchmark.drop_duplicates(\"screen_file\")\n",
    "if not os.path.exists(\"data/eval/summaries.npy\"):\n",
    "    client = OpenAI(base_url=api_url, api_key=api_key)\n",
    "    summary_prompt_files = {\n",
    "        \"cell\": \"prompts/summary-cell.json\",\n",
    "        \"phenotype\": \"prompts/summary-phenotype.json\",\n",
    "    }\n",
    "\n",
    "    summaries = dict()\n",
    "    for col, prompt_file in summary_prompt_files.items():\n",
    "        summaries[col] = dict()\n",
    "        with open(prompt_file) as f:\n",
    "            prompt_template = f.read()\n",
    "        for i, experiment in experiments.iterrows():\n",
    "            term = experiment[col]\n",
    "            prompt = json.loads(re.sub(\"\\{.*\\}\", term, prompt_template))\n",
    "            completion = client.chat.completions.create(\n",
    "                messages=prompt,\n",
    "                model=summary_model,\n",
    "                seed=42,\n",
    "                n=1,\n",
    "                temperature=0,\n",
    "                max_tokens=2048,\n",
    "            )\n",
    "            summary = completion.choices[0].message.content\n",
    "            summaries[col][term] = summary\n",
    "    np.save(\"data/eval/summaries.npy\", summaries)\n",
    "else:\n",
    "    summaries = np.load(\"data/eval/summaries.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    not os.path.exists(\"data/eval/summary_embeddings.npy\") or\n",
    "    not os.path.exists(\"data/eval/term_embeddings.npy\")\n",
    "):\n",
    "    term_embeddings = dict()\n",
    "    summary_embeddings = dict()\n",
    "    client = OpenAI(base_url=api_url, api_key=api_key)\n",
    "    for col, term_summaries in summaries.items():\n",
    "        term_embeddings[col] = dict()\n",
    "        summary_embeddings[col] = dict()\n",
    "        for term, summary in term_summaries.items():\n",
    "            out = client.embeddings.create(\n",
    "                input=[term, summary],\n",
    "                model=embedding_model,\n",
    "            )\n",
    "            term_emb = np.asarray(out.data[0].embedding)\n",
    "            summ_emb = np.asarray(out.data[1].embedding)\n",
    "            term_embeddings[col][term] = term_emb\n",
    "            summary_embeddings[col][term] = summ_emb\n",
    "    np.save(\"data/eval/summary_embeddings.npy\", summary_embeddings)\n",
    "    np.save(\"data/eval/term_embeddings.npy\", term_embeddings)\n",
    "else:\n",
    "    summary_embeddings = np.load(\"data/eval/summary_embeddings.npy\", allow_pickle=True).item()\n",
    "    term_embeddings = np.load(\"data/eval/term_embeddings.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classification Over Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_genome = pd.read_csv(\"../genomes/genome_homo_sapiens.tsv\", sep=\"\\t\")\n",
    "human_genome = human_genome[human_genome[\"Gene_Type\"] == \"PROTEIN_CODING\"].reset_index(drop=True)\n",
    "human_genome[\"OFFICIAL_SYMBOL\"] = human_genome[\"OFFICIAL_SYMBOL\"].str.lower()\n",
    "mouse_genome = pd.read_csv(\"../genomes/genome_mus_musculus.tsv\", sep=\"\\t\")\n",
    "mouse_genome = mouse_genome[mouse_genome[\"Gene_Type\"] == \"PROTEIN_CODING\"].reset_index(drop=True)\n",
    "mouse_genome[\"OFFICIAL_SYMBOL\"] = mouse_genome[\"OFFICIAL_SYMBOL\"].str.lower()\n",
    "\n",
    "genome_map = {\n",
    "    \"human\": human_genome,\n",
    "    \"mouse\": mouse_genome,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_model = \"q33613fx\"\n",
    "unsumm_model = \"v2zz1ph7\"\n",
    "\n",
    "def load_model(use_summarized):\n",
    "    if use_summarized:\n",
    "        sd = torch.load(f\"runs/classifier-summarized/{summ_model}/VirtualCRISPR/{summ_model}/checkpoints/last.ckpt\", map_location=\"cpu\")\n",
    "    else:\n",
    "        sd = torch.load(f\"runs/classifier-unsummarized/{unsumm_model}/VirtualCRISPR/{unsumm_model}/checkpoints/last.ckpt\", map_location=\"cpu\")\n",
    "    sd = {k.replace(\"classifier.mlp.\", \"\"): v for k, v in sd[\"state_dict\"].items()}\n",
    "\n",
    "    cls = MLP(\n",
    "        input_dim=3072*4,\n",
    "        reduction_factor=2,\n",
    "        n_hidden=4,\n",
    "        output_dim=2,\n",
    "    )\n",
    "    cls.load_state_dict(sd)\n",
    "    cls.eval()\n",
    "\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for label, use_summarized in [(\"Summarized\", True), (\"Unsummarized\", False)]:\n",
    "    print(f\"================= {label} =================\")\n",
    "    cls = load_model(use_summarized)\n",
    "    preds = []\n",
    "    trues = []\n",
    "    for (_, method, cell, organism, phenotype), _hits in benchmark.groupby([\"screen_file\", \"perturbation\", \"cell\", \"organism\", \"phenotype\"]):\n",
    "        method = method.title()\n",
    "        genome = genome_map[organism]\n",
    "\n",
    "        if use_summarized:\n",
    "            me = torch.as_tensor(summ_method_emb[method], dtype=torch.float32)\n",
    "            pe = torch.as_tensor(summary_embeddings[\"phenotype\"][phenotype], dtype=torch.float32)\n",
    "            ce = torch.as_tensor(summary_embeddings[\"cell\"][cell], dtype=torch.float32)\n",
    "        else:\n",
    "            me = torch.as_tensor(method_emb[method], dtype=torch.float32)\n",
    "            pe = torch.as_tensor(term_embeddings[\"phenotype\"][phenotype], dtype=torch.float32)\n",
    "            ce = torch.as_tensor(term_embeddings[\"cell\"][cell], dtype=torch.float32)\n",
    "\n",
    "        X = []\n",
    "        for gene, hit in zip(_hits[\"gene\"], _hits[\"hit\"]):\n",
    "            gene_id = genome.loc[genome[\"OFFICIAL_SYMBOL\"] == gene.lower(), \"IDENTIFIER_ID\"].iloc[0]\n",
    "            if use_summarized:\n",
    "                ge = summ_gene_emb_map[organism][gene_id]\n",
    "            else:\n",
    "                ge = gene_emb_map[organism][gene_id]\n",
    "            ge = torch.as_tensor(ge, dtype=torch.float32)\n",
    "            X.append(torch.concat([me, ce, pe, ge], dim=0))\n",
    "            trues.append(hit)\n",
    "        X = torch.stack(X)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            logits = cls(X)\n",
    "        pred = softmax(logits, dim=1)\n",
    "        preds.extend(pred[:, 1].numpy())\n",
    "    trues = np.asarray(trues)\n",
    "    preds = np.asarray(preds)\n",
    "    results[label] = (trues, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = dict()\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "for label, (trues, preds) in results.items():\n",
    "    fpr, tpr, thresholds = roc_curve(trues, preds)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, label=f\"{label} (AUROC={auroc:0.3f})\")\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "    auprc = auc(recall, precision)\n",
    "    ax2.plot(recall, precision, label=f\"{label} (AUPRC={auprc:0.3f})\")\n",
    "\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    preds_bin = (preds > optimal_threshold).astype(int)\n",
    "\n",
    "    true_neg_pred_neg = ((trues == 0) & (preds_bin == 0)).sum()\n",
    "    true_neg_pred_pos = ((trues == 0) & (preds_bin == 1)).sum()\n",
    "    true_pos_pred_neg = ((trues == 1) & (preds_bin == 0)).sum()\n",
    "    true_pos_pred_pos = ((trues == 1) & (preds_bin == 1)).sum()\n",
    "\n",
    "    tp = true_pos_pred_pos\n",
    "    tn = true_neg_pred_neg\n",
    "    fp = true_neg_pred_pos\n",
    "    fn = true_pos_pred_neg\n",
    "\n",
    "    metrics[label] = {\n",
    "        \"n-binary-pred\": tp + tn + fp + fn,\n",
    "        \"f1\": (2 * tp) / (2 * tp + fp + fn),\n",
    "        \"ppv\": tp / (tp + fp),\n",
    "        \"npv\": tn / (fn + tn),\n",
    "        \"sensitivity\": tp / (tp + fn),\n",
    "        \"specificity\": tn / (fp + tn),\n",
    "        \"fpr\": fp / (fp + tn),\n",
    "        \"auroc\": auroc,\n",
    "        \"auprc\": auprc,\n",
    "    }\n",
    "\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], label=f\"Random (AUROC=0.500)\", linestyle=\"dashed\", color=\"red\", alpha=0.5)\n",
    "pr_chance = trues.sum() / len(trues)\n",
    "ax2.plot([0, 1], [pr_chance, pr_chance], label=f\"Random (AUPRC={pr_chance:0.3f})\", linestyle=\"dashed\", color=\"red\", alpha=0.5)\n",
    "\n",
    "ax1.legend(title=\"Embeddings\")\n",
    "ax1.set(\n",
    "    xlabel=\"1 - Specificity\",\n",
    "    ylabel=\"Sensitivity\",\n",
    "    title=\"CRISPR Classifier: ROC\",\n",
    ")\n",
    "\n",
    "ax2.legend(title=\"Embeddings\")\n",
    "ax2.set(\n",
    "    xlabel=\"Recall\",\n",
    "    ylabel=\"Precision\",\n",
    "    title=\"CRISPR Classifier: PR\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(metrics).T[[\"auroc\", \"auprc\", \"f1\", \"fpr\"]].to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crispr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
